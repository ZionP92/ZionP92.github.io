---
layout: post
title: 선형 회귀 기초
date: 2022-12-08 19:30:30 +0900
category: ML
---

환경은 구글 Colab의 Jupyter Notebook이다. <br /> 
아래의 코드는 스파르타 코딩 클럽의 1주차 숙제를 바탕으로 구성되어진 것들이다.

```py
import os
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam, SGD
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
```

<br />

Kaggle에서 API를 받고 아래와 같이 기입해주면 된다.

```py
os.environ['KAGGLE_USERNAME'] = 'MY_USERNAME'
os.environ['KAGGLE_KEY'] = '###########MY_KEY###############'
```

<br />

데이터 셋을 kaggle에서 받아 오는데 쉘 명령어 기입은 .ipynb 파일에서 이루어지기에 !를 앞에 붙여야 한다.
아래는 1주차 숙제 파트인 급여와 경력 데이터이다.
```bash
!kaggle datasets download -d rsadiq/salary
!unzip salary.zip
```

<br />

어떻게 생겼는지 보고
```py
df = pd.read_csv('Salary.csv')
df.head(10)
```
<img src="../../../../img/ML/basics_001.png" width=200>

<br />

얼마나 되는지 확인하고
```py
print(df.shape)
```

<img src="../../../../img/ML/basics_002.png" width=80>

<br />

그래프로 그려 본다.
```py
sns.pairplot(df, x_vars = ['YearsExperience'], y_vars = ['Salary'], height=5)
```

<img src="../../../../img/ML/basics_003.png" width=360>

<br />

Keras에 맞게 np.array를 이용해 변환해준다, float32가 사용되니 주의할 것
```py
x_data = np.array(df[['YearsExperience']], dtype=np.float32)
y_data = np.array(df['Salary'], dtype=np.float32)
print(x_data.shape)
print(y_data.shape)
```

<img src="../../../../img/ML/basics_004.png" width=80>

결과 값이 하나가 콤마로 끝나는데 x는 dataframe인데 y를 series 형태로 불러와서 그렇다, 실험해보니 둘다 dataframe 형식으로 해도 별 에러 없이 잘 된다 하지만 내 생각에 여기는 지금 한정적 데이터만 가지고하니 아무래도 상관없지만, 그냥 하라는대로 후보정하는 편이 편할 것 같다. <br />
둘이 형식이 맞게 reshape을 이용해 고쳐준다, -1은 알아서 맞추라는 뜻이고 뒤의 1은 1로 정해준 것이다.
```py
x_data = x_data.reshape((-1, 1))
y_data = y_data.reshape((-1, 1))
print(x_data.shape)
print(y_data.shape)
```

<img src="../../../../img/ML/basics_005.png" width=80>

<br />

강의에선 한번 분할만 예시로 알려주는데 삼분할 하는 것이 좋다고도 하였다
그래서 구글링하여 알아낸 바로 Train, Validation, Test를 분할하여 진행하였다.
분할에는 sklearn을 이용, 단순히 두번에 걸쳐 분할하면 된다.(딱히 한번에 두 번 쪼개는 건 없는 듯)
아래는 처음 분할시 20% 정도로 나누고 이차 분할에 80%의 25% 
즉, 전체의 20% 정도로 검증과 테스트 세트를 균일히 나눈 방법이다.<br />
추가로 random_state는 색인 쯤으로 여기면 될 것 같다. 아무 번호나 기입하여 사용하면 되고 매번 무작위가 되는 것을 방지하여 같은 번호를 넣으면 같은 결과가 나온다.

```py
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=30)
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=30)
print(x_train.shape, x_val.shape, x_test.shape)
print(y_train.shape, y_val.shape, y_test.shape)
```

<img src="../../../../img/ML/basics_006.png" width=200>

<br />

Dense layer가 정확히 무엇을 하는지 확실히 이해하지 못하여 구글링을 해본 결과, 완벽한 이해라고는 할 수 없지만 대략 다각적 조건들의 여파로 인한 결과를 유추해 내기 위한 것 같다. 아직까지는 노드 한 개 정도로만 사용하고 있으니 앞으로 실습을 해보며 좀 더 깔끔하게 이해해야 할 것 같다. <br />
급여 특성상 단위가 커서 mean_squared_error을 이용하니 무지막지하게 큰 단위들이 나와 mean_absolute_error을 대신 사용하였고 optimizer로는 Adam을 사용하는 것을 예제로 많이 봤는데 여기선 한번 SGD를 사용해 보았다, Adam은 학습율을 조정하여 유동적으로 결과 값을 잡아내는데 일반적으론 SGD 보다 훌륭하나 이미지 쪽에선 SGD보다 안좋다는 얘기가 있다. SGD는 추계를 바탕으로 하는데 SGD에서 파생되어 후발주자로 나온게 Adam인 만큼 Adam을 많이 쓰는 것 같다. <br />
역시 급여 단위가 단위인 만큼 Learning_rate를 100으로 잡았고 epochs는 데이터가 얼마 안되니 좀 넉넉히 잡았다.

```py
model = Sequential([Dense(1)])
model.compile(loss='mean_absolute_error', optimizer=SGD(learning_rate=100))
model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=1500)
```

<img src="../../../../img/ML/basics_007.png" width=800>

위는 학습 마지막 부분인데, 한 epcho 850 정도 될 때부터 거의 변화가 없어졌다, 1000번만 잡아도 됐을 뻔 했다.

<br />

예측 값은 붉은 색으로 정해준다.

```py
y_pred = model.predict(x_test)

plt.scatter(x_test, y_test)
plt.scatter(x_test, y_pred, color='r')
plt.show()
```

<img src="../../../../img/ML/basics_008.png" width=420>

결과 값이 얼추 비슷하게 나왔다 역시 사람이든 기계든 정확하게 많이 배워야 한다.

<br />

전에 Coursera에서 데이터 사이언스를 공부할 때 다루어 본 것들이 있어서 그런건지 아직 첫 주라 그런건지 크게 어려운 부분은 없는 것 같다.