---
layout: post
title: 33개 논문의 사적 축약
date: 2023-01-11 17:23:44 +0900
category: ML
---
# &nbsp;&nbsp;&nbsp;앞서는 말
<br />
머신러닝에 대한 폭 넓은 이해도를 가지고자 예전부터 궁금했던 패스트캠퍼스의 '33개 프로젝트로 완성하는 컴퓨터비전 딥러닝 심화 과정'이라는 강의를 수강 중이다. (광고 아님 학습하는 33개 논문 선정 출처를 위해 남김)

<br />

강의는 Colab으로 진행되나 난 내 기기로 충분하기에 Colab에 쓸 돈을 아끼려 여러모로 복붙한 코드를 환경에 맞게 설정하고 디버깅 등만을 하였으며 또 강의에 쓰인 코드들을 여기에 적으면 문제가 될지도 모르니 비록 비교적 어렵게 디버깅하며 배운 부분도 많았지만 코드 스니펫은 넣지 않고 여기선 논문에 대한 자세함보다는 최대한 내 심플한 핵심적 이해와 생각을 적으려 한다. 본인 개발 학습 일지일 뿐 오류가 있을 수 있으니 참고하시길

<br />

* # Faster RCNN (Faster Region-Based Convolutional Neural Network)

RCNN - Fast RCNN - Faster RCNN으로 성능의 변천을 겪은 모델이다. 

RCNN은 이미지 분류를 하는 CNN에 Region이라는 개념을 도입하여 물체 감지가 가능하게 만들었다. 여기서 Region은 Selective serch라는 알고리즘을 통해 정한다. Selective search는 픽셀을 기준으로 단순화를 시켜 국소적으로 묶는 방법이다.

Fast RCNN은 Selective search의 Region proposals를 CNN에 물리는게 아니라 CNN을 이용하여 대상 이미지에서 Convolutional feature map을 출력하여 특정 관심 부분에서만 Selective search를 진행함으로 간소화시켰다.

Faster RCNN은 Network에 알고리즘으로 Region proposals를 학습하게 하여  Region proposal network를 만들어 시간소요가 많이되는 Selective search 알고리즘을 대체함으로 더욱 빨리지게 하였다.

<br />

* # SSD (Single Shot Detector)

SSD는 비율로 정해진 기본 박스들을 가지고 Feature map 사이즈를 바꿔가며 대입하여 NMS(non-maximum suppression)로 confidence 값이 제일 높은 특정 박스만 남겨 감지하는 방법으로 Faster RCNN보다 빠르고 성능이 좋다.

<br />

* # YOLO (You Look Only Once)

YOLO는 이미지를 Grid로 분할하여 각 분할된 격자 안에서 심플한 CNN으로 Bounding box를 잡고 해당 격자 안의 가장 높은 Confidence를 보이는 물체를 그 격자에 할당 라벨로 하며 기본적으로 해상도를 낮게 하여 진행해 속도가 굉장히 빠른편이나 속도에 비해 정확도가 높을 뿐이지 다른 모델에 비하여 정확도가 좀 떨어지는 경향이 있다. 하지만 다른 모델을 저해상도나 단순화등을 하면 YOLO에 비해 정확도가 현저히 떨어져 실시간 감지등 여러 방면에서 쓰임이 있다. 특징으로는 오류에서 위치가 차지하는 부분이 많은데 이는 내 생각엔 격자를 바탕으로 하니 태생적으로 있을 수 밖에 없는 문제인 것 같다.

<br /><br />
# &nbsp;&nbsp;&nbsp;나가는 말
<br />

아직 배우는 중이라 지속적으로 업데이트 할 계획입니다. 앞서 말한대로 개인적으로 축약 정리한 요지이고 자세한 내용은 다루지 않을 생각이니 궁금하거나 참고할 부분있으시다면 달아놓은 논문 링크를 통하여 보시기 바랍니다.

<br />

|논문|링크|
|:--|:--|
|RCNN|<https://arxiv.org/pdf/1311.2524.pdf>|
|Fast RCNN|<https://arxiv.org/pdf/1504.08083.pdf>|
|Faster RCNN|<https://arxiv.org/pdf/1506.01497.pdf>|
|SSD|<https://arxiv.org/pdf/1512.02325.pdf>|
|YOLO|<https://openaccess.thecvf.com/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf>|